---
title: "model selection"
author: "Nils Wendel Heinrich"
date: "2024-08-19"
output: pdf_document
---

```{r global, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup, include=FALSE}

library(tidyverse)
library(lme4)
library(lmerTest)
library(sjPlot)
library(latex2exp)
library(ggplot2)
library(fitdistrplus)
library(simr)
library(arrow)
library(MASS)
library(MuMIn)

set.seed(36)
N_iterations <- 10000

```


```{r data, echo=FALSE}

#paths
trial_data_path <- paste(getwd(), "/data/trialwise_data.csv", sep="")
questionnaire_data_path <- paste(getwd(), "/data/questionnaire_scores.csv", sep="")

#load data
trial_data <- read_csv(trial_data_path)
questionnaire_data <- read_csv(questionnaire_data_path)

#factorization
trial_data$feedback <- factor(trial_data$feedback, levels = c("neutral", "positive", "negative"))
trial_data$input_noise_magnitude <- factor(trial_data$input_noise_magnitude)
trial_data$code <- factor(trial_data$code) #added as factor variable by Maren
trial_data$inpnoise_ascending <- factor(trial_data$inpnoise_ascending)
trial_data$block <- as.numeric(trial_data$block)

```

Create one dataset containing both the experimental data and the questionnaire scores.
Furthermore, exclude participant 12 and 14 from all statistical analyses.
```{r create final dataset, echo=FALSE}

#exclude pp 12 and 14 from experimental data
trial_data <- trial_data %>% filter(code != 12 & code != 14)

#rename "ID" to "code"
colnames(questionnaire_data)[colnames(questionnaire_data) == "ID"] <- "code"
all_data <- merge(trial_data, questionnaire_data, by = "code", all.x = TRUE)

#add categorical depression variable with participants scorint >= 16 (critical) scoring 1 and participants scoring < 16 scoring 0
all_data$CESDR_cat <- ifelse(all_data$CESDR >= 16, 1, 0)
all_data$CESDR_cat <- as.factor(all_data$CESDR_cat )

```

# Model selection for a linear model predicting performance (average distance to line throughout trial)

### Distributional analysis:
```{r performance_boxcox, echo=FALSE}

lambda_performance <- boxcox(lm(all_data$avg_dist_trialwise ~ 1))

lambda_performance$x[which(lambda_performance$y == max(lambda_performance$y))]

```

referring to:
https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/

lambda, the expected value is close to 0.0, implying a log transformation.

### null model to explore random intercept effects
predict mean (independent of any predictors) performance rating
```{r performance_lm, echo=FALSE}

performance_null <- lmer(log(avg_dist_trialwise) ~ 1 + (1|code), data = all_data)
summary(performance_null)

```

How much variance in performance is explained by the individual random intercept effect *code*?
```{r performance.r^2, echo=FALSE}

# marginal R^2 loaded on code
0.02931**2

```

## Exploring fixed effects by likelihood ratio tests

We start with the most complex fixed effects structure (simply throwing fixed effects in the model that are specified by our hypotheses). Then we will test this model against less complex ones (where we eliminate individual fixed effects). For the model that predicts performance that is not overly complex...
```{r most_complex_model, echo=FALSE}

model1.complex <- lmer(log(avg_dist_trialwise) ~ input_noise_magnitude * block
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model1.complex)

```
And we only find significant effects. We can still try to eliminate the interaction effect and see what happens.

```{r model1.1, echo=FALSE}

model1.1 <- lmer(log(avg_dist_trialwise) ~ input_noise_magnitude + block
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model1.1)

```

```{r likelihoodratio test, echo=FALSE}

anova(model1.1, model1.complex)

```
We detect significance. That means that if we were to eliminate the interaction term, we would take out something crucial that reduces predictive power of our model. We will thus stick to model1.complex.

## Exploring random slope effects by referring to BIC
Now that we identified the fixed effects in our model we can work on the random effects structure. When it comes to selecting random slope effects though, the likelihood ratio test won't be sufficient anymore (not for comparing models with different random effects structures). Random slopes "open up" the fixed effects for the different groups of our random intercept effects: they split the model apart by introducing a lot more parameters. We can select random slope effects by referring to an **information criterion**. I usually use the **Bayes information criterion (BIC)**. It penalizes the number of data points used to fit the model (on top of the number of parameters). I like the idea of accounting for overfitting when selecting models. (An alternative to the BIC is the **Akaike information criterion (AIC)**, which only penalizes the number of parameters.) 

Here we will start with the most complex random effects structure and reduce the complexity further and further until we don't detect singularity anymore or the BIC won't go smaller anymore (smaller BICs are preferred).

Just entering all the fixed effects as random slopes.
```{r complex_random_effects, echo=FALSE}

model1.2 <- lmer(log(avg_dist_trialwise) ~ input_noise_magnitude * block
                 + (1 + input_noise_magnitude * block|code),
                 data = all_data, REML = FALSE)

```
Failed to converge. Eliminating the interaction term in the random slope structure.

```{r model1.3, echo=FALSE}

model1.3 <- lmer(log(avg_dist_trialwise) ~ input_noise_magnitude * block
                 + (1 + input_noise_magnitude + block|code),
                 data = all_data, REML = FALSE)

```

Also failed to converge. Only entering single random slope effects.

```{r model1.in, echo=FALSE}

model1.in <- lmer(log(avg_dist_trialwise) ~ input_noise_magnitude * block
                 + (1 + input_noise_magnitude|code),
                 data = all_data, REML = FALSE)

```

```{r model1.bl, echo=FALSE}

model1.bl <- lmer(log(avg_dist_trialwise) ~ input_noise_magnitude * block
                 + (1 + block|code),
                 data = all_data, REML = FALSE)

```

Let's compare these with the random intercept only model:

```{r model_comparison, echo=FALSE}

anova(model1.complex, model1.in, model1.bl)

```
model1.in, the model with input noise as random slope effect wins (it haas the smallest BIC). Now we can take a look at the main effects.

```{r model1.summary, echo=FALSE}

summary(model1.in)

```

### Transform estimates back
means:
```{r transform means, echo=FALSE}

exp(fixef(model1.in)[1]) # intercept
exp(fixef(model1.in)[2]) # input noise
exp(abs(fixef(model1.in)[3])) # block
exp(fixef(model1.in)[4]) # input noise * block

```
standard errors:
```{r transform sds, echo=FALSE}
print("intercept:")
exp(summary(model1.in)$coefficients[1, "Std. Error"]) # intercept

print("input_noise_magnitude2:")
exp(summary(model1.in)$coefficients[2, "Std. Error"]) # input noise

print("block:")
exp(summary(model1.in)$coefficients[3, "Std. Error"]) # block

print("input_noise_magnitude2*block:")
exp(summary(model1.in)$coefficients[4, "Std. Error"]) # input noise * block

```

...and run simulations based on the final selected model...

## Generaring simulations based on the final selected model

parametric bootstrap:
```{r bootstrap, echo=FALSE}

confint(model1.in, nsim=N_iterations, parm=c('input_noise_magnitude2', 'block', 'input_noise_magnitude2:block'), method='boot')

```

```{r transform CIs, echo=FALSE}

print("input_noise_magnitude2:")
exp(0.533586052)
exp(0.63558477)

print("block:")
-exp(abs(-0.023676032))
-exp(abs(-0.01747090))

print("input_noise_magnitude2*block:")
exp(0.006779193)
exp(0.01549945)

```

"Compared to input noise magnitude 0.5, input noise magnitude 2.0 significantly increased the average distance to the line followed throughout a trial (beta=1.794, sigma=1.026, CI=[1.705, 1.888],  p<.001)."




# Model selection for a linear model predicting SoC

### Distributional analysis:
```{r soc_boxcox, echo=FALSE}

lambda_soc <- boxcox(lm(all_data$SoC ~ 1))

lambda_soc$x[which(lambda_soc$y == max(lambda_soc$y))]

```
referring to:
https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/

lambda, the expected value is closer to 0.5 than 1 (1 would be ideal, suggesting no transformation). 0.5 implies a âˆš transformation, which we will apply. 
"a boxcox distributional analysis implied a square root transformation of the predicted variable"


### null model to explore random intercept effects
predict mean (independent of any predictors) SoC rating
```{r soc_lm, echo=FALSE}
soc_null <- lmer(sqrt(SoC) ~ 1 + (1|code), data = all_data)
summary(soc_null)

```
How much variance in SoC Ratings is explained by the individual random intercept effects?
```{r soc.r^2, echo=FALSE}

# marginal R^2 loaded on code
0.03609**2

```

## Exploring fixed effects by likelihood ratio tests

We start with the most complex fixed effects structure (simply throwing fixed effects in the model that are specified by our hypotheses). Then we will test this model against less complex ones (where we eliminate individual fixed effects).
```{r most_complex_model, echo=FALSE}

model2.complex <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*ExternalLC
                      + feedback*InternalLC
                      + feedback*input_noise_magnitude
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.complex)

```
We see no significance for several of the effects. This is where we can start. I will target the interactions first and try keeping the main effects.

Eliminating the interaction term between feedback and input_noise_magnitude:
```{r model2.1, echo=FALSE}

model2.1 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                + feedback*ExternalLC
                + feedback*InternalLC
                + feedback*CESDR
                + input_noise_magnitude*block
                + (1|code),
                data = all_data, REML = FALSE)

summary(model2.1)

```

```{r sanity_check, echo=FALSE}

loglik.model2.1 <- logLik(model2.1)

loglik.model2.complex <- logLik(model2.complex)

# remember to put the less complex model first...
loglik_teststatistic <- -2 * (loglik.model2.1[1] - loglik.model2.complex[1])

# in df we have to state the difference in the degrees of freedom:
# model.1 df = 18
# model.complex df = 20
# 20 - 18 = 2
p.value <- pchisq(loglik_teststatistic, df = 2, lower.tail = FALSE)

```
The p value is way above 0.05. This tells us that we fail to reject the null hypothesis. The models are "similar" meaning that we can use the less complex model and reduce the degrees of freedom (number of parameters).

Now with the package...
```{r likelihoodratio test, echo=FALSE}

anova(model2.1, model2.complex)

```
Checks out, same results! The Pr(>Chisq) is not significant, telling us that the models are not significantly different from another. This means we can use the less complex model (if we reduce the number of parameters by throwing out the interaction term, we're not loosing critical predictive power).

Proceeding with model2.1 and trying to eliminate further interaction terms. Here we try to eliminate the interaction between feedback and InternalLC. It also had no significance whatsoever above.
```{r model2.2, echo=FALSE}

model2.2 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                + feedback*ExternalLC
                + feedback*CESDR
                + input_noise_magnitude*block
                + (1|code),
                data = all_data, REML = FALSE)

summary(model2.2)

```

```{r likelihoodratio test, echo=FALSE}

anova(model2.2, model2.1)

```
Again the test statistic tells us that there is no significant difference. Proceeding with model2.2

Now we target the interaction between feedback and ExternalLC and try to eliminate this one.
```{r model2.3, echo=FALSE}

model2.3 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                + feedback*CESDR
                + input_noise_magnitude*block
                + (1|code),
                data = all_data, REML = FALSE)

summary(model2.3)

```

```{r likelihoodratio test, echo=FALSE}

anova(model2.3, model2.2)

```

Again no difference between the models, so we can kick the interaction term and don't loose critical predictive power. Proceeding with model2.3.

We will target our first main effect: InternalLC:
```{r model2.4, echo=FALSE}

model2.4 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + feedback*CESDR
                + input_noise_magnitude*block
                + (1|code),
                data = all_data, REML = FALSE)

summary(model2.4)

```

```{r likelihoodratio test, echo=FALSE}

anova(model2.4, model2.3)

```

Yep we can safely kick InternalLC. Proceeding with model2.4.

Now it get's a little more tricky. We find no significance for the main effects of CESDR and also no significance for one of its interaction effects. But the other interaction effect is significant... Is it safe to eliminate the whole interaction term or the main effect? We'll see.

Eliminating the complete interaction term:
```{r model2.5, echo=FALSE}

model2.5 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                           + input_noise_magnitude*block
                           + (1|code),
                           data = all_data, REML = FALSE)

summary(model2.5)

```

```{r likelihoodratio test, echo=FALSE}

anova(model2.5, model2.4)

```
We see significance. This means that the models are significantly different from another in their predictive power and we shouldn't just throw out the interaction term. But because it's the feedback:positive * CESDR -interaction that is the weak point, we can create columns in our data set that are separate for *negative*  and *positive* feedback.

```{r individual_feedback columns, echo=FALSE}

all_data <- all_data %>% 
  mutate(negative_feedback=case_when(
    feedback=="negative" ~ 1,
    feedback=="positive" | feedback=="neutral" ~ 0
  ))

all_data <- all_data %>% 
  mutate(positive_feedback=case_when(
    feedback=="positive" ~ 1,
    feedback=="negative" | feedback=="neutral" ~ 0
  ))

```

Now we put the individual columns in the model but leave out the interaction between positive_feedback and CESDR:
```{r model2.6, echo=FALSE}

model2.6 <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1|code),
                data = all_data, REML = FALSE)

summary(model2.6)

```

```{r likelihoodratio test, echo=FALSE}

anova(model2.6, model2.4)

```
We can safely proceed with model2.6.

There is no significant main effect of CESDR, but it is involved in a significant interaction effect and it's a numeric variable. We will just leave the effect in the model. This is our final fixed effects structure. Now we can start to explore random effects structure.

## Exploring random slope effects by referring to BIC
Now that we identified the fixed effects in our model we can work on the random effects structure. When it comes to selecting random slope effects though, the likelihood ratio test won't be sufficient anymore (not for comparing models with different random effects structures). Random slopes "open up" the fixed effects for the different groups of our random intercept effects: they split the model apart by introducing a lot more parameters. We can select random slope effects by referring to an **information criterion**. I usually use the **Bayes information criterion (BIC)**. It penalizes the number of data points used to fit the model (on top of the number of parameters). I like the idea of accounting for overfitting when selecting models. (An alternative to the BIC is the **Akaike information criterion (AIC)**, which only penalizes the number of parameters.) 

Here we will start with the most complex random effects structure and reduce the complexity further and further until we don't detect singularity anymore or the BIC won't go smaller anymore (smaller BICs are preferred).

Just entering all the fixed effects as random slopes.
```{r complex_random_effects, echo=FALSE}

model2.7 <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise +     negative_feedback*CESDR + input_noise_magnitude*block|code),
                data = all_data, REML = FALSE)

```
That took a while and the model is drastically overparameterized (failed to converge)... We will first eliminate interaction effects.

eliminating negative_feedback*CESDR
```{r complex_random_effects, echo=FALSE}

model2.8 <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise + input_noise_magnitude*block|code),
                data = all_data, REML = FALSE)

```
Detecting singularity...

Throwing out input_noise_magnitude*block.
```{r complex_random_effects, echo=FALSE}

model2.8 <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise|code),
                data = all_data, REML = FALSE)

```

Still singular. Hmmmm maybe we should start keeping only a single random slope effect. Starting with input_noise_magnitude.

```{r random_slope.input_noise, echo=FALSE}

model2.in <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + input_noise_magnitude|code),
                data = all_data, REML = FALSE)

```

That one worked. Ok than let's just build single random slope models and compare those.

```{r random_slope.nfeedback, echo=FALSE}

model2.nfeedback <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback|code),
                data = all_data, REML = FALSE)

```

```{r random_slope.pfeedback, echo=FALSE}

model2.pfeedback <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + positive_feedback|code),
                data = all_data, REML = FALSE)

```

```{r random_slope.ExternalLC, echo=FALSE}

model2.ExternalLC <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + ExternalLC|code),
                data = all_data, REML = FALSE)

```
That one is singular. We will omit it in the model comparison.

```{r random_slope.CESDR, echo=FALSE}

model2.CESDR <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + CESDR|code),
                data = all_data, REML = FALSE)

```

Also singular.

```{r random_slope.block, echo=FALSE}

model2.block <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + block|code),
                data = all_data, REML = FALSE)

```

```{r random_slope.avg_dist_trialwise, echo=FALSE}

model2.avg_dist_trialwise <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + avg_dist_trialwise|code),
                data = all_data, REML = FALSE)

```

That one actually failed to converge

Let's compare the non-singular models and the random intercept only one.
```{r model_comparison, echo=FALSE}

anova(model2.6, model2.in, model2.nfeedback, model2.pfeedback, model2.block)

```
The only statistic of interest for us in this output is the BIC and we're searching for the smallest BIC. model2.in (input_noise_magnitude as random slope effect) has the lowest BIC even outcompeting the random intercept only model.

We can try to add additional random slope effects now. Starting with the interaction: input_noise_magnitude*block
```{r random_slope.input_noise*block, echo=FALSE}

model2.inb <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + input_noise_magnitude*block|code),
                data = all_data, REML = FALSE)

```

Failed to converge again. Maybe without the interaction and just main effects?

```{r random_slope.input_noise+block, echo=FALSE}

model2.inb <- lmer(sqrt(SoC) ~ negative_feedback + positive_feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + input_noise_magnitude+block|code),
                data = all_data, REML = FALSE)

```

Nope. Let's take model2.in as our final model as adding further random slopes will only end up in a worse fit. Now we can take our first look at the effects.

```{r final selected model, echo=FALSE}

summary(model2.in)

```

### Back-transformation

means:
```{r transform means, echo=FALSE}

fixef(model2.in)[1]**2 # intercept
fixef(model2.in)[2]**2 # negative_feedback
fixef(model2.in)[3]**2 # positive_feedback
fixef(model2.in)[4]**2 # input_noise_magnitude2
fixef(model2.in)[5]**2 # ExternalLC
fixef(model2.in)[6]**2 # CESDR
fixef(model2.in)[7]**2 # block
fixef(model2.in)[8]**2 # avg_dist_trialwise
fixef(model2.in)[9]**2 # negative_feedback:CESDR
fixef(model2.in)[10]**2 # input_noise_magnitude2:block

```

standard errors:
```{r transform sds, echo=FALSE}

print("intercept:")
(summary(model2.in)$coefficients[1, "Std. Error"])**2

print("negative_feedback:")
(summary(model2.in)$coefficients[2, "Std. Error"])**2

print("positive_feedback:")
(summary(model2.in)$coefficients[3, "Std. Error"])**2

print("input_noise_magnitude2:")
(summary(model2.in)$coefficients[4, "Std. Error"])**2

print("ExternalLC:")
(summary(model2.in)$coefficients[5, "Std. Error"])**2

print("CESDR:")
(summary(model2.in)$coefficients[6, "Std. Error"])**2

print("block:")
(summary(model2.in)$coefficients[7, "Std. Error"])**2

print("avg_dist_trialwise:")
(summary(model2.in)$coefficients[8, "Std. Error"])**2

print("negative_feedback*CESDR:")
(summary(model2.in)$coefficients[9, "Std. Error"])**2

print("input_noise_magnitude2*block:")
(summary(model2.in)$coefficients[10, "Std. Error"])**2

```

## Generaring simulations based on the final selected model

parametric bootstrap:
```{r bootstrap, echo=FALSE}

confint(model2.in, nsim=N_iterations, parm=c('negative_feedback', 'positive_feedback', 'input_noise_magnitude2', 'ExternalLC', 'CESDR', 'block', 'avg_dist_trialwise', 'negative_feedback:CESDR', 'input_noise_magnitude2:block'), method='boot')

```

"...bounds of the 95% confidence interval were obtained by a parametric bootstrap with {N_iterations} iterations."

```{r transform CIs, echo=FALSE}

print("negative_feedback:")
-(0.099079119)**2
-(0.064289019)**2

print("positive_feedback:")
(0.025204015)**2
(0.047975512)**2

print("input_noise_magnitude2:")
-(0.371335049)**2
-(0.193213762)**2

print("ExternalLC:")
(0.041552809)**2
(0.164827711)**2

print("CESDR:")
-(0.005585758)**2
(0.004406567)**2

print("block:")
(0.003797198)**2
(0.010063422)**2

print("avg_dist_trialwise:")
-(0.009427785)**2
-(0.008545179)**2

print("negative_feedback*CESDR:")
(0.001079844)**2
(0.003229721)**2

print("input_noise_magnitude2*block:")
-(0.023712643)**2
-(0.014914305)**2

```

reporting our effect:
...compared to input noise magnitude=0.5, increasing the magnitude of input noise to 2.0 significantly decreases SoC (beta=0.080, sigma=0.002, CI=[-0.138, -0.037],  p<.001).

