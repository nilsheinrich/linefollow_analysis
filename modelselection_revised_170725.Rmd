---
title: "model selection - revised"
author: "Maren Giersiepen"
date: "2025-07-17"
output: pdf_document
---

This script contains the revised statistical analysis of data reported in the Manuscript "Am I in control? The dynamics of sensory information, performance feedback, and personality in shaping the sense of control". 
Revisions built upon valuable suggestions received from two anonymous reviewers during the peer-review process. 
Specifically, compared to the original analysis, reported in model_selection.Rmd, the current script contains additional interactions and follow-up analyses for a model predicting participants' sense of control using different low-level and high-level cues of control.

```{r global, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup, include=FALSE}

library(tidyverse)
library(lme4)
library(lmerTest)
library(sjPlot)
library(latex2exp)
library(ggplot2)
library(fitdistrplus)
library(simr)
library(arrow)
library(MASS)
library(MuMIn)

set.seed(36)
N_iterations <- 10000

```


```{r data, echo=FALSE}
#paths
trial_data_path <- paste(getwd(), "/data/trialwise_data.csv", sep="")
questionnaire_data_path <- paste(getwd(), "/data/questionnaire_scores.csv", sep="")

#load data
trial_data <- read_csv(trial_data_path)
questionnaire_data <- read_csv(questionnaire_data_path)

#factorization
trial_data$feedback <- factor(trial_data$feedback, levels = c("neutral", "positive", "negative"))
trial_data$input_noise_magnitude <- factor(trial_data$input_noise_magnitude)
trial_data$code <- factor(trial_data$code) #added as factor variable by Maren
trial_data$inpnoise_ascending <- factor(trial_data$inpnoise_ascending)
trial_data$block <- as.numeric(trial_data$block)

```

Create one dataset containing both the experimental data and the questionnaire scores.
Furthermore, exclude participant 12 and 14 from all statistical analyses.
```{r create final dataset, echo=FALSE}

#exclude pp 12 and 14 from experimental data
trial_data <- trial_data %>% filter(code != 12 & code != 14)

#rename "ID" to "code"
colnames(questionnaire_data)[colnames(questionnaire_data) == "ID"] <- "code"
all_data <- merge(trial_data, questionnaire_data, by = "code", all.x = TRUE)

#add categorical depression variable with participants scorint >= 16 (critical) scoring 1 and participants scoring < 16 scoring 0
all_data$CESDR_cat <- ifelse(all_data$CESDR >= 16, 1, 0)
all_data$CESDR_cat <- as.factor(all_data$CESDR_cat )

```

# Model selection for a linear model predicting SoC

### Distributional analysis:
```{r soc_boxcox, echo=FALSE}

lambda_soc <- boxcox(lm(all_data$SoC ~ 1))

lambda_soc$x[which(lambda_soc$y == max(lambda_soc$y))]

```
referring to:
https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/

lambda, the expected value is closer to 0.5 than 1 (1 would be ideal, suggesting no transformation). 0.5 implies a square root transformation, which we will apply. 
"a boxcox distributional analysis implied a square root transformation of the predicted variable"


### null model to explore random intercept effects
predict mean (independent of any predictors) SoC rating
```{r soc_lm, echo=FALSE}
soc_null <- lmer(sqrt(SoC) ~ 1 + (1|code), data = all_data)
summary(soc_null)

```

What is the mean and std of SoC given our null model?
```{r square transform estimates, echo=FALSE}

print(sprintf("estimate: %.2f", 1.93698**2))
print(sprintf("Std. Error: %.2f", 0.02771**2))

```

How much variance in SoC Ratings is explained solely by the random intercept effect code?
```{r soc.r^2, echo=FALSE}

# marginal R^2 loaded on code
0.03609 / (0.03609+0.18223)

```

## Exploring fixed effects by likelihood ratio tests

We start with the most complex fixed effects structure (simply throwing fixed effects in the model that are specified by our hypotheses). Then we will test this model against less complex ones (where we eliminate individual fixed effects).
```{r most_complex_model, echo=FALSE}

model2.complex <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*feedback*CESDR #extension
                      + input_noise_magnitude*feedback*InternalLC #extension
                      + input_noise_magnitude*feedback*ExternalLC
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.complex)

```
We see no significance for several of the effects. This is where we can start. I will target the interactions first and try keeping the main effects.

Eliminating the interaction term between feedback, input_noise_magnitude, InternalLC
```{r model2.1, echo=FALSE}


model2.1 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*InternalLC
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*InternalLC #extension
                      + input_noise_magnitude*feedback*CESDR #extension
                      #+ input_noise_magnitude*feedback*InternalLC #extension
                      + input_noise_magnitude*feedback*ExternalLC
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.1)

```
Compare model fit complex vs. 2.1
```{r likelihoodratio test2.1, echo=FALSE}

anova(model2.1, model2.complex)

```
The Pr(>Chisq) is not significant, telling us that the models are not significantly different from another. This means we can use the less complex model (if we reduce the number of parameters by throwing out the interaction term, we're not loosing critical predictive power).

Eliminating the interaction term between feedback, input_noise_magnitude, CESDR
```{r model2.2, echo=FALSE}


model2.2 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*InternalLC
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*InternalLC #extension
                      #+ input_noise_magnitude*feedback*CESDR #extension
                      + input_noise_magnitude*feedback*ExternalLC
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.2)

```

Compare model fit 2.2 vs. 2.1
```{r likelihoodratio test2.2, echo=FALSE}

anova(model2.2, model2.1)

```
The p-value is not significant. We can proceed without the three-way interaction. 

Lets test whether excluding the 3-way interaction of input_noise, feedback, ExternalLC improves model fit

```{r model2.3, echo=FALSE}

model2.3 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*ExternalLC
                      + feedback*InternalLC
                      + feedback*input_noise_magnitude
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + input_noise_magnitude*InternalLC #extension
                      #+ input_noise_magnitude*feedback*ExternalLC
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.3)

```

Compare model fit 2.3 vs. 2.2
```{r likelihoodratio test2.3, echo=FALSE}

anova(model2.3, model2.2)

```
The p-value is again > .05. Therefore, we proceed with a model only including 2-way interactions.


Eliminating the interaction term between feedback and input_noise_magnitude:
```{r model2.4, echo=FALSE}

model2.4 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*ExternalLC
                      + feedback*InternalLC
                      #+ feedback*input_noise_magnitude
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + input_noise_magnitude*InternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.4)

```

Compare model fit 2.4 vs. 2.3
```{r likelihoodratio test2.4, echo=FALSE}

anova(model2.4, model2.3)

```
We can savely proceed with the simpler model, as the p-value is way above .05.

Proceeding with model2.4 and trying to eliminate further interaction terms. Here we try to eliminate the interaction between feedback and InternalLC. It also had no significance whatsoever above.
```{r model2.5, echo=FALSE}

model2.5 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*ExternalLC
                      #+ feedback*InternalLC
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + input_noise_magnitude*InternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.5)

```

Compare model fit 2.5 vs. 2.4
```{r likelihoodratio test2.5, echo=FALSE}

anova(model2.5, model2.4)

```
Again the test statistic tells us that there is no significant difference. Proceeding with model2.5

Now we target the interaction between feedback and ExternalLC and try to eliminate this one.
```{r model2.6, echo=FALSE}

model2.6 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      #+ feedback*ExternalLC
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + input_noise_magnitude*InternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.6)

```

Compare model fit 2.6 vs. 2.5
```{r likelihoodratio test2.6, echo=FALSE}

anova(model2.6, model2.5)

```

Again no difference between the models, so we can kick the interaction term and don't loose critical predictive power. Proceeding with model2.6.

Now, exclude the non-significant interaction of Internal LC and input_noise
```{r model2.7, echo=FALSE}

model2.7 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      #+ input_noise_magnitude*InternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.7)

```

Compare model fit 2.7 vs. 2.6
```{r likelihoodratio test2.7, echo=FALSE}

anova(model2.7, model2.6)

```
We proceed with the simpler model, as p>.05.

We will target our first main effect: InternalLC:
```{r model2.8, echo=FALSE}

model2.8 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.8)

```

Compare model fit 2.8 vs. 2.7
```{r likelihoodratio test2.8, echo=FALSE}

anova(model2.8, model2.7)

```
Yep we can safely kick InternalLC. Proceeding with model2.8.

Now it get's a little more tricky. We find no significance for the main effects of CESDR and also no significance for one of its interaction effects. But the other interaction effect is significant... Is it safe to eliminate the whole interaction term or the main effect? We'll see.

Eliminating the complete interaction term CESDR*Feedback
```{r model2.9, echo=FALSE}

model2.9 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      #+ feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.9)

```

Compare model fit 2.9 vs. 2.8
```{r likelihoodratio test2.9, echo=FALSE}

anova(model2.9, model2.8)

```
We see significance. This means that the models are significantly different from another in their predictive power and we shouldn't just throw out the interaction term. 

We also see a significant interaction of positive feedback and block but not of negative feedback and block. Should we exclude this interaction from model 2.8?

Eliminating the complete interaction term block*Feedback
```{r model2.10, echo=FALSE}

model2.10 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      #+ block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.10)

```

Compare model fit 2.10 vs. 2.8
```{r likelihoodratio test2.10, echo=FALSE}

anova(model2.10, model2.8)

```
We again see that excluding the entire interaction term significantly reduces model fit. We therefore proceed with model 2.8 as the final fixed effects model.
Note that this model also includes a non-significant main effect of External LC. As this term is part of a significant interaction though, it is kept in the model.

```{r final fixed effects model, echo=FALSE}

summary(model2.8)

```
This is our final fixed effects structure. Now we can start to explore random effects structure.

## Exploring random slope effects by referring to BIC
Now that we identified the fixed effects in our model we can work on the random effects structure. When it comes to selecting random slope effects though, the likelihood ratio test won't be sufficient anymore (not for comparing models with different random effects structures). Random slopes "open up" the fixed effects for the different groups of our random intercept effects: they split the model apart by introducing a lot more parameters. We can select random slope effects by referring to an **information criterion**. I usually use the **Bayes information criterion (BIC)**. It penalizes the number of data points used to fit the model (on top of the number of parameters). I like the idea of accounting for overfitting when selecting models. (An alternative to the BIC is the **Akaike information criterion (AIC)**, which only penalizes the number of parameters.) 

Here we will start with the most complex random effects structure and reduce the complexity further and further until we don't detect singularity anymore or the BIC won't go smaller anymore (smaller BICs are preferred).

Just entering all the ws-fixed effects as random slopes.
```{r complex_random_effects, model2.8_1, echo=FALSE}

model2.8_1 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude + block + avg_dist_trialwise + input_noise_magnitude*block + block*feedback|code),
                      data = all_data, REML = FALSE)
summary(model2.8_1)

```
That took a while and the model is drastically overparameterized (is singular)... We will first eliminate interaction effects.

eliminating block*feedback
```{r random_effects_2, model2.8_2, echo=FALSE}

model2.8_2 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude + block + avg_dist_trialwise + input_noise_magnitude*block|code),
                      data = all_data, REML = FALSE)
summary(model2.8_2)

```
Model failed to converge. Reduce random slopes further...

eliminating input_noise_magnitude*block
```{r random_effects_3, model2.8_3, echo=FALSE}

model2.8_3 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude + block + avg_dist_trialwise |code),
                      data = all_data, REML = FALSE)
summary(model2.8_3)

```
Model failed to converge. We now exclude random slopes not included in a BS X WS interaction.

eliminating avg_dist_trialwise
```{r random_effects_4, model2.8_4, echo=FALSE}

model2.8_4 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude + block |code),
                      data = all_data, REML = FALSE)
summary(model2.8_4)

```
Model failed to converge. 

Eliminate random slope of block.
```{r random_effects_5, model2.8_5, echo=FALSE}

model2.8_5 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude|code),
                      data = all_data, REML = FALSE)
summary(model2.8_5)

```
model is singular. 

Test model with individual random slopes for feedback and input noise. 

only keep random slope for feedback
```{r random_slope.fb, echo=FALSE}

model2.8_fb <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback|code),
                      data = all_data, REML = FALSE)
summary(model2.8_fb)

```
Also singular.

only keep random slope for input_noise
```{r random_slope.fb, echo=FALSE}

model2.8_in <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + input_noise_magnitude|code),
                      data = all_data, REML = FALSE)
summary(model2.8_in)

```
This one converges. Lets compare the non-singular model with that without a random slope. 

```{r model_comparison2, echo=FALSE}

anova(model2.8, model2.8_in)

```
The only statistic of interest for us in this output is the BIC and we're searching for the smallest BIC. The model including a random slope for input noise fits the data significantly better than the model without this slope. 

model2.8_in is our final model.

```{r final selected model2, echo=FALSE}

summary(model2.8_in)

```

### Back-transformation

means:
```{r transform means2, echo=FALSE}

fixef(model2.8_in)[1]**2 # intercept
fixef(model2.8_in)[2]**2 # positive_feedback
-fixef(model2.8_in)[3]**2 # negative_feedback
-fixef(model2.8_in)[4]**2 # input_noise_magnitude2
fixef(model2.8_in)[5]**2 # ExternalLC
fixef(model2.8_in)[6]**2 # CESDR
fixef(model2.8_in)[7]**2 # block
-fixef(model2.8_in)[8]**2 # avg_dist_trialwise
-fixef(model2.8_in)[9]**2 # positive_feedback:CESDR
fixef(model2.8_in)[10]**2 # negative_feedback:CESDR
-fixef(model2.8_in)[11]**2 #input noise:block
-fixef(model2.8_in)[12]**2 #feedbackpositive:block
fixef(model2.8_in)[13]**2 #feedbacknegative:block
-fixef(model2.8_in)[14]**2 #input_noise:CESDR
fixef(model2.8_in)[15]**2 #input noise:ExternalLC

```

standard errors:
```{r transform sds2, echo=FALSE}

print("intercept:")
(summary(model2.8_in)$coefficients[1, "Std. Error"])**2

print("positive_feedback:")
(summary(model2.8_in)$coefficients[2, "Std. Error"])**2

print("negative_feedback:")
(summary(model2.8_in)$coefficients[3, "Std. Error"])**2

print("input_noise_magnitude2:")
(summary(model2.8_in)$coefficients[4, "Std. Error"])**2

print("ExternalLC:")
(summary(model2.8_in)$coefficients[5, "Std. Error"])**2

print("CESDR:")
(summary(model2.8_in)$coefficients[6, "Std. Error"])**2

print("block:")
(summary(model2.8_in)$coefficients[7, "Std. Error"])**2

print("avg_dist_trialwise:")
(summary(model2.8_in)$coefficients[8, "Std. Error"])**2

print("positive_feedback*CESDR:")
(summary(model2.8_in)$coefficients[9, "Std. Error"])**2

print("negative_feedback*CESDR:")
(summary(model2.8_in)$coefficients[10, "Std. Error"])**2

print("input_noise_magnitude2*block:")
(summary(model2.8_in)$coefficients[11, "Std. Error"])**2

print("positive_feedback*block:")
(summary(model2.8_in)$coefficients[12, "Std. Error"])**2

print("negative_feedback*block:")
(summary(model2.8_in)$coefficients[13, "Std. Error"])**2

print("input_noise_magnitude2*CESDR:")
(summary(model2.8_in)$coefficients[14, "Std. Error"])**2

print("input_noise_magnitude2*ExternalLC:")
(summary(model2.8_in)$coefficients[15, "Std. Error"])**2

```
## Generating simulations based on the final selected model

parametric bootstrap:
```{r bootstrap, include=FALSE}

bootstrap_results <- bootMer(
  model2.8_in,
  FUN = function(model) fixef(model),  # extract fixed effects
  nsim = N_iterations,                 # number of bootstrap samples
  use.u = FALSE,                       # refit the model for each sample
  type = "parametric"                  # bootstrapping type
)
# convert bootstrap estimates to a data frame
bootstrap_df <- as.data.frame(bootstrap_results$t)
colnames(bootstrap_df) <- names(fixef(model2.8_in))  # assign variable names

# save to CSV
write.csv(bootstrap_df, "bootstrap_results_revised_model.csv", row.names = FALSE)
```

parametric bootstrap:
```{r bootstrap2, include=FALSE}

#confint(model2.8_in, nsim=N_iterations, parm=c('feedbackpositive', 'feedbacknegative', 'input_noise_magnitude2', 'ExternalLC', 'CESDR', 'block', 'avg_dist_trialwise', 'feedbackpositive:CESDR', #'feedbacknegative:CESDR','input_noise_magnitude2:block', 'feedbackpositive:block', 'feedbacknegative:block', 'input_noise_magnitude2:CESDR', 'input_noise_magnitude2:ExternalLC'), method='boot')

```

Results:
                                          2.5 %        97.5 %
feedbackpositive                   5.866217e-02  0.1180111522
feedbacknegative                  -1.215921e-01 -0.0624626506
input_noise_magnitude2            -5.940525e-01 -0.1216488362
ExternalLC                        -2.934276e-02  0.1238230001
CESDR                              5.751026e-05  0.0124864176
block                              5.328455e-03  0.0139202633
avg_dist_trialwise                -9.406878e-03 -0.0085236969
feedbackpositive:CESDR            -1.512891e-03  0.0009227763
feedbacknegative:CESDR             7.794146e-04  0.0032266508
input_noise_magnitude2:block      -2.370946e-02 -0.0149202516
feedbackpositive:block            -1.569780e-02 -0.0055849958
feedbacknegative:block            -2.350713e-03  0.0077347537
input_noise_magnitude2:CESDR      -2.277708e-02 -0.0065617561
input_noise_magnitude2:ExternalLC  1.754305e-02  0.2192967353


"...bounds of the 95% confidence interval were obtained by a parametric bootstrap with {N_iterations} iterations."

```{r transform CIs2, echo=FALSE}

print("positive_feedback:")
(5.866217e-02)**2
(0.1180111522)**2

print("negative_feedback:")
-(-1.215921e-01 )**2
-(-0.0624626506)**2

meaprint("input_noise_magnitude2:")
-(-5.940525e-01)**2
-( -0.1216488362)**2

print("ExternalLC:")
-(-2.934276e-02)**2
(0.1238230001)**2

print("CESDR:")
(5.751026e-05)**2
(0.0124864176)**2

print("block:")
( 5.328455e-03)**2
(0.0139202633)**2

print("avg_dist_trialwise:")
-( -9.406878e-03)**2
-(-0.0085236969)**2

print("feedbacknegative:CESDR:")
(7.794146e-04)**2
(0.0032266508)**2

print("feedbackpositive:CESDR:")
-(-1.512891e-03)**2
(0.0009227763)**2

print("input_noise_magnitude2*block:")
-(-2.370946e-02)**2
-(-0.0149202516)**2

print("feedbackpositive:block:")
-(-1.569780e-02)**2
-(-0.0055849958)**2

print("feedbacknegative:block:")
-(-2.350713e-03 )**2
(0.0077347537)**2

print("input_noise_magnitude2:ExternalLC:")
(1.754305e-02)**2
( 0.2192967353)**2

print("input_noise_magnitude2:CESDR:")
-(-2.277708e-02)**2
-(-0.0065617561)**2
```
reporting our effect:
...compared to input noise magnitude=0.5, increasing the magnitude of input noise to 2.0 significantly decreases SoC (beta=-0.080, sigma=0.002, CI=[-0.138, -0.037],  p<.001).

```{r arrow_file, include=FALSE}
# write to arrow data file
#write_feather(all_data, "data/all_data.arrow")
```

# Post-hoc tests to compare effect sizes

## negative vs. positive feedback
```{r negFeedback vs. posFeedback, include=TRUE}

bootstrap_data <- read_csv("C:/Users/maren/Documents/GitHub/linefollow_analysis/data/bootstrap_results_revised_model.csv")

negative_feedback.diffs <- abs(bootstrap_data$feedbacknegative)
positive_feedback.diffs <- abs(bootstrap_data$feedbackpositive)

t.test(negative_feedback.diffs, positive_feedback.diffs, alternative = "two.sided", paired=TRUE)
# we're testing whether the distribution of negative_feedback.diffs is different from that of positive_feedback.diffs

mean(bootstrap_data$feedbacknegative)
mean(bootstrap_data$feedbackpositive)


```
The effect of negative FB on SoC is signficantly larger than the effect of positive feedback on SoC. 

## input noise vs. negative feedback
```{r inputNoise vs. negFeedback, include=TRUE}

input_noise.diffs <- abs(bootstrap_data$input_noise_magnitude2)
negative_feedback.diffs <- abs(bootstrap_data$feedbacknegative)
positive_feedback.diffs <- abs(bootstrap_data$feedbackpositive)

#negative fb
t.test(input_noise.diffs, negative_feedback.diffs, alternative = "two.sided", paired=TRUE)
# we're testing whether the distribution of input_noise.diffs is smaller than  negative_feedback.diffs

#positive fb
t.test(input_noise.diffs, positive_feedback.diffs, alternative = "two.sided", paired=TRUE)

#correct alpha values for multiple comparison *2 => p <.2.2e-16**2 = p < .001

mean(bootstrap_data$input_noise_magnitude2)
mean(bootstrap_data$feedbacknegative)
mean(bootstrap_data$feedbackpositive)

```
The effect of input noise on SOC is significantly stronger than the effect of either positive or negative feedback.

## input noise:CESDR vs. negative feedback:CESDR
```{r inputNoise*CESDR vs. CESDR*negFeedback, include=TRUE}

CESDR_FBneg.diffs <- abs(bootstrap_data$`feedbacknegative:CESDR`)
CESDR_inpnoise.diffs <- abs(bootstrap_data$`input_noise_magnitude2:CESDR`)

#negative fb
t.test(CESDR_inpnoise.diffs, CESDR_FBneg.diffs, alternative = "two.sided", paired=TRUE)
# we're testing whether the distribution of CESDR_FBneg.diffs is different to CESDR_inpnoise.diffs

mean(CESDR_FBneg.diffs)
mean(CESDR_inpnoise.diffs)

```
The effect of CESDR on the effect of Input noise is significantly stronger than the effect of CESDR on negative feedback.

## correlational analyses CESD-R score and feedback effect size
Analyses are performed to follow-up BSxWS interactions. We extract the effect size of the WS-effect and correlate it with its BS-interaction counterpart. 
This allows us to examine whether the interactions are driven by a different size of the effect between individuals, or a more general effect of individual differences on the effect of the WS-factor on SoC. 

```{r follow-up correlations CESDR and negFeedback effect, include=TRUE}
 
#correlate each participants' average neutral-negative SoC difference with CESD-R score
pp_wise_negeff <- all_data %>%
  group_by(code,feedback, CESDR) %>%
  summarise(avg_soc = mean(SoC, na.rm = TRUE), .groups = "drop")

avg_soc_wide_fb <- pp_wise_negeff %>%
  pivot_wider(names_from = feedback, values_from = avg_soc, names_prefix = "mean_SoC_")

avg_soc_wide_fb <- avg_soc_wide_fb %>%
  mutate(diff_negneu = mean_SoC_neutral - mean_SoC_negative)

cor.test(avg_soc_wide_fb$CESDR, avg_soc_wide_fb$diff_negneu)

```
The effect of negative feedback [neutral-negative] on SoC decreases with increasing depression scores. 

## correlational analyses CESD-R score X input noise  & Correlational analyses External LC score X input noise
```{r inputNoise*CESDR vs. CESDR*negFeedback, include=TRUE}
 
#correlate each participants' average low-high input noise SoC difference with CESD-R score
pp_wise_inpNoise_eff <- all_data %>%
  group_by(code,input_noise_magnitude, CESDR, ExternalLC) %>%
  summarise(avg_soc_inpn = mean(SoC, na.rm = TRUE), .groups = "drop")

avg_soc_wide_inpnoise <- pp_wise_inpNoise_eff %>%
  pivot_wider(names_from = input_noise_magnitude, values_from = avg_soc_inpn, names_prefix = "mean_SoC_")

avg_soc_wide_inpnoise <- avg_soc_wide_inpnoise %>%
  mutate(diff_low_high = mean_SoC_0.5 - mean_SoC_2)

cor.test(avg_soc_wide_inpnoise$CESDR, avg_soc_wide_inpnoise$diff_low_high)

#correlate each participants' average neutral-negative SoC difference with ExternalLC score
cor.test(avg_soc_wide_inpnoise$ExternalLC, avg_soc_wide_inpnoise$diff_low_high)

#correlate participants' average SoC score (across input noise conditions) with external LC score
avg_soc_wide_inpnoise <- avg_soc_wide_inpnoise %>%
  mutate(mean_soc_inpnoise = (mean_SoC_0.5 + mean_SoC_2)/2)

cor.test(avg_soc_wide_inpnoise$ExternalLC, avg_soc_wide_inpnoise$mean_soc_inpnoise)
```


