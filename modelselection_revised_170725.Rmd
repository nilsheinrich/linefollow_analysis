---
title: "model selection - revised"
author: "Maren Giersiepen"
date: "2025-07-17"
output: pdf_document
---

```{r global, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup, include=FALSE}

library(tidyverse)
library(lme4)
library(lmerTest)
library(sjPlot)
library(latex2exp)
library(ggplot2)
library(fitdistrplus)
library(simr)
library(arrow)
library(MASS)
library(MuMIn)

set.seed(36)
N_iterations <- 10000

```


```{r data, echo=FALSE}
#set path
setwd("C:/Users/maren/Documents/GitHub/linefollow_analysis/")

#paths
trial_data_path <- paste("C:/Users/maren/Documents/GitHub/linefollow_analysis/data/trialwise_data.csv", sep="")
questionnaire_data_path <- paste("C:/Users/maren/Documents/GitHub/linefollow_analysis/data/questionnaire_scores.csv", sep="")

#load data
trial_data <- read_csv(trial_data_path)
questionnaire_data <- read_csv(questionnaire_data_path)

#factorization
trial_data$feedback <- factor(trial_data$feedback, levels = c("neutral", "positive", "negative"))
trial_data$input_noise_magnitude <- factor(trial_data$input_noise_magnitude)
trial_data$code <- factor(trial_data$code) #added as factor variable by Maren
trial_data$inpnoise_ascending <- factor(trial_data$inpnoise_ascending)
trial_data$block <- as.numeric(trial_data$block)

```

Create one dataset containing both the experimental data and the questionnaire scores.
Furthermore, exclude participant 12 and 14 from all statistical analyses.
```{r create final dataset, echo=FALSE}

#exclude pp 12 and 14 from experimental data
trial_data <- trial_data %>% filter(code != 12 & code != 14)

#rename "ID" to "code"
colnames(questionnaire_data)[colnames(questionnaire_data) == "ID"] <- "code"
all_data <- merge(trial_data, questionnaire_data, by = "code", all.x = TRUE)

#add categorical depression variable with participants scorint >= 16 (critical) scoring 1 and participants scoring < 16 scoring 0
all_data$CESDR_cat <- ifelse(all_data$CESDR >= 16, 1, 0)
all_data$CESDR_cat <- as.factor(all_data$CESDR_cat )

```

# Model selection for a linear model predicting SoC

### Distributional analysis:
```{r soc_boxcox, echo=FALSE}

lambda_soc <- boxcox(lm(all_data$SoC ~ 1))

lambda_soc$x[which(lambda_soc$y == max(lambda_soc$y))]

```
referring to:
https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/

lambda, the expected value is closer to 0.5 than 1 (1 would be ideal, suggesting no transformation). 0.5 implies a square root transformation, which we will apply. 
"a boxcox distributional analysis implied a square root transformation of the predicted variable"


### null model to explore random intercept effects
predict mean (independent of any predictors) SoC rating
```{r soc_lm, echo=FALSE}
soc_null <- lmer(sqrt(SoC) ~ 1 + (1|code), data = all_data)
summary(soc_null)

```

What is the mean and std of SoC given our null model?
```{r square transform estimates, echo=FALSE}

print(sprintf("estimate: %.2f", 1.93698**2))
print(sprintf("Std. Error: %.2f", 0.02771**2))

```

How much variance in SoC Ratings is explained solely by the random intercept effect code?
```{r soc.r^2, echo=FALSE}

# marginal R^2 loaded on code
0.03609 / (0.03609+0.18223)

```

## Exploring fixed effects by likelihood ratio tests

We start with the most complex fixed effects structure (simply throwing fixed effects in the model that are specified by our hypotheses). Then we will test this model against less complex ones (where we eliminate individual fixed effects).
```{r most_complex_model, echo=FALSE}

model2.complex <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*feedback*CESDR #extension
                      + input_noise_magnitude*feedback*InternalLC #extension
                      + input_noise_magnitude*feedback*ExternalLC
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.complex)

```
We see no significance for several of the effects. This is where we can start. I will target the interactions first and try keeping the main effects.

Eliminating the interaction term between feedback, input_noise_magnitude, InternalLC
```{r model2.1, echo=FALSE}


model2.1 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*InternalLC
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*InternalLC #extension
                      + input_noise_magnitude*feedback*CESDR #extension
                      #+ input_noise_magnitude*feedback*InternalLC #extension
                      + input_noise_magnitude*feedback*ExternalLC
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.1)

```
Compare model fit complex vs. 2.1
```{r likelihoodratio test2.1, echo=FALSE}

anova(model2.1, model2.complex)

```
The Pr(>Chisq) is not significant, telling us that the models are not significantly different from another. This means we can use the less complex model (if we reduce the number of parameters by throwing out the interaction term, we're not loosing critical predictive power).

Eliminating the interaction term between feedback, input_noise_magnitude, CESDR
```{r model2.2, echo=FALSE}


model2.2 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*InternalLC
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*InternalLC #extension
                      #+ input_noise_magnitude*feedback*CESDR #extension
                      + input_noise_magnitude*feedback*ExternalLC
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.2)

```

Compare model fit 2.2 vs. 2.1
```{r likelihoodratio test2.2, echo=FALSE}

anova(model2.2, model2.1)

```
The p-value is not significant. We can proceed without the three-way interaction. 

Lets test whether excluding the 3-way interaction of input_noise, feedback, ExternalLC improves model fit

```{r model2.3, echo=FALSE}

model2.3 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*ExternalLC
                      + feedback*InternalLC
                      + feedback*input_noise_magnitude
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + input_noise_magnitude*InternalLC #extension
                      #+ input_noise_magnitude*feedback*ExternalLC
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.3)

```

Compare model fit 2.3 vs. 2.2
```{r likelihoodratio test2.3, echo=FALSE}

anova(model2.3, model2.2)

```
The p-value is again > .05. Therefore, we proceed with a model only including 2-way interactions.


Eliminating the interaction term between feedback and input_noise_magnitude:
```{r model2.4, echo=FALSE}

model2.4 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*ExternalLC
                      + feedback*InternalLC
                      #+ feedback*input_noise_magnitude
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + input_noise_magnitude*InternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)

summary(model2.4)

```

Compare model fit 2.4 vs. 2.3
```{r likelihoodratio test2.4, echo=FALSE}

anova(model2.4, model2.3)

```
We can savely proceed with the simpler model, as the p-value is way above .05.

Proceeding with model2.4 and trying to eliminate further interaction terms. Here we try to eliminate the interaction between feedback and InternalLC. It also had no significance whatsoever above.
```{r model2.5, echo=FALSE}

model2.5 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*ExternalLC
                      #+ feedback*InternalLC
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + input_noise_magnitude*InternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.5)

```
Compare model fit 2.5 vs. 2.4
```{r likelihoodratio test2.5, echo=FALSE}

anova(model2.5, model2.4)

```
Again the test statistic tells us that there is no significant difference. Proceeding with model2.5

Now we target the interaction between feedback and ExternalLC and try to eliminate this one.
```{r model2.6, echo=FALSE}

model2.6 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      #+ feedback*ExternalLC
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + input_noise_magnitude*InternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.6)

```

Compare model fit 2.6 vs. 2.5
```{r likelihoodratio test2.3, echo=FALSE}

anova(model2.6, model2.5)

```

Again no difference between the models, so we can kick the interaction term and don't loose critical predictive power. Proceeding with model2.6.

Now, exclude the non-significant interaction of Internal LC and input_noise
```{r model2.7, echo=FALSE}

model2.7 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      #+ input_noise_magnitude*InternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.7)

```
Compare model fit 2.7 vs. 2.6
```{r likelihoodratio test2.7, echo=FALSE}

anova(model2.7, model2.6)

```
We proceed with the simpler model, as p>.05.

We will target our first main effect: InternalLC:
```{r model2.8, echo=FALSE}

model2.8 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.8)

```

Compare model fit 2.8 vs. 2.7
```{r likelihoodratio test2.8, echo=FALSE}

anova(model2.8, model2.7)

```
Yep we can safely kick InternalLC. Proceeding with model2.8.

Now it get's a little more tricky. We find no significance for the main effects of CESDR and also no significance for one of its interaction effects. But the other interaction effect is significant... Is it safe to eliminate the whole interaction term or the main effect? We'll see.

Eliminating the complete interaction term CESDR*Feedback
```{r model2.9, echo=FALSE}

model2.9 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      #+ feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.9)

```
Compare model fit 2.9 vs. 2.8
```{r likelihoodratio test2.9, echo=FALSE}

anova(model2.9, model2.8)

```
We see significance. This means that the models are significantly different from another in their predictive power and we shouldn't just throw out the interaction term. 

We also see a significant interaction of positive feedback and block but not of negative feedback and block. Should we exclude this interactoin from model 2.8?

Eliminating the complete interaction term block*Feedback
```{r model2.10, echo=FALSE}

model2.10 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      #+ block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1|code),
                      data = all_data, REML = FALSE)
summary(model2.10)

```

Compare model fit 2.10 vs. 2.8
```{r likelihoodratio test2.10, echo=FALSE}

anova(model2.10, model2.8)

```
We again see that excluding the entire interaction term significantly reduces model fit. We therefore proceed with model 2.8 as the final fixed effects model.
Note that this model also includes a non-significant main effect of External LC. As this term is part of a significant interaction though, it is kept in the model.

```{r final fixed effects model, echo=FALSE}

summary(model2.8)

```
This is our final fixed effects structure. Now we can start to explore random effects structure.

## Exploring random slope effects by referring to BIC
Now that we identified the fixed effects in our model we can work on the random effects structure. When it comes to selecting random slope effects though, the likelihood ratio test won't be sufficient anymore (not for comparing models with different random effects structures). Random slopes "open up" the fixed effects for the different groups of our random intercept effects: they split the model apart by introducing a lot more parameters. We can select random slope effects by referring to an **information criterion**. I usually use the **Bayes information criterion (BIC)**. It penalizes the number of data points used to fit the model (on top of the number of parameters). I like the idea of accounting for overfitting when selecting models. (An alternative to the BIC is the **Akaike information criterion (AIC)**, which only penalizes the number of parameters.) 

Here we will start with the most complex random effects structure and reduce the complexity further and further until we don't detect singularity anymore or the BIC won't go smaller anymore (smaller BICs are preferred).

Just entering all the ws-fixed effects as random slopes.
```{r complex_random_effects, model2.8_1, echo=FALSE}

model2.8_1 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude + block + avg_dist_trialwise + input_noise_magnitude*block + block*feedback|code),
                      data = all_data, REML = FALSE)
summary(model2.8_1)

```
That took a while and the model is drastically overparameterized (is singular)... We will first eliminate interaction effects.

eliminating block*feedback
```{r random_effects_2, model2.8_2, echo=FALSE}

model2.8_2 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude + block + avg_dist_trialwise + input_noise_magnitude*block|code),
                      data = all_data, REML = FALSE)
summary(model2.8_2)

```
Model failed to converge. Reduce random slopes further...

eliminating input_noise_magnitude*block
```{r random_effects_3, model2.8_3, echo=FALSE}

model2.8_3 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude + block + avg_dist_trialwise |code),
                      data = all_data, REML = FALSE)
summary(model2.8_3)

```
Model failed to converge. We now exclude random slopes not included in a BS X WS interaction.

eliminating avg_dist_trialwise
```{r random_effects_4, model2.8_4, echo=FALSE}

model2.8_4 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude + block |code),
                      data = all_data, REML = FALSE)
summary(model2.8_4)

```
Model failed to converge. 

Eliminate random slope of block.
```{r random_effects_5, model2.8_5, echo=FALSE}

model2.8_5 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback + input_noise_magnitude|code),
                      data = all_data, REML = FALSE)
summary(model2.8_5)

```
model is singular. 

Test model with individual random slopes for feedback and input noise. 

only keep random slope for feedback
```{r random_slope.fb, echo=FALSE}

model2.8_fb <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + feedback|code),
                      data = all_data, REML = FALSE)
summary(model2.8_fb)

```
Also singular.

only keep random slope for inpput_noise
```{r random_slope.fb, echo=FALSE}

model2.8_in <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise
                      + feedback*CESDR
                      + input_noise_magnitude*block
                      + block*feedback #extension
                      + input_noise_magnitude*CESDR #extension
                      + input_noise_magnitude*ExternalLC #extension
                      + (1 + input_noise_magnitude|code),
                      data = all_data, REML = FALSE)
summary(model2.8_in)

```
This one converges. Lets compare the non-singular model with that without a random slope. 

```{r model_comparison2, echo=FALSE}

anova(model2.8, model2.8_in)

```
The only statistic of interest for us in this output is the BIC and we're searching for the smallest BIC. The model including a random slope for input noise fits the data significantly better than the model without this slope. 

model2.8_in is our final model.

```{r final selected model2, echo=FALSE}

summary(model2.8_in)

```

### Back-transformation

means:
```{r transform means2, echo=FALSE}

fixef(model2.8_in)[1]**2 # intercept
fixef(model2.8_in)[2]**2 # positive_feedback
-fixef(model2.8_in)[3]**2 # negative_feedback
-fixef(model2.8_in)[4]**2 # input_noise_magnitude2
fixef(model2.8_in)[5]**2 # ExternalLC
fixef(model2.8_in)[6]**2 # CESDR
fixef(model2.8_in)[7]**2 # block
-fixef(model2.8_in)[8]**2 # avg_dist_trialwise
-fixef(model2.8_in)[9]**2 # positive_feedback:CESDR
fixef(model2.8_in)[10]**2 # negative_feedback:CESDR
-fixef(model2.8_in)[11]**2 #input noise:block
-fixef(model2.8_in)[12]**2 #feedbackpositive:block
fixef(model2.8_in)[13]**2 #feedbacknegative:block
-fixef(model2.8_in)[14]**2 #input_noise:CESDR
fixef(model2.8_in)[15]**2 #input noise:ExternalLC

```

standard errors:
```{r transform sds2, echo=FALSE}

print("intercept:")
(summary(model2.8_in)$coefficients[1, "Std. Error"])**2

print("positive_feedback:")
(summary(model2.8_in)$coefficients[2, "Std. Error"])**2

print("negative_feedback:")
(summary(model2.8_in)$coefficients[3, "Std. Error"])**2

print("input_noise_magnitude2:")
(summary(model2.8_in)$coefficients[4, "Std. Error"])**2

print("ExternalLC:")
(summary(model2.8_in)$coefficients[5, "Std. Error"])**2

print("CESDR:")
(summary(model2.8_in)$coefficients[6, "Std. Error"])**2

print("block:")
(summary(model2.8_in)$coefficients[7, "Std. Error"])**2

print("avg_dist_trialwise:")
(summary(model2.8_in)$coefficients[8, "Std. Error"])**2

print("positive_feedback*CESDR:")
(summary(model2.8_in)$coefficients[9, "Std. Error"])**2

print("negative_feedback*CESDR:")
(summary(model2.8_in)$coefficients[10, "Std. Error"])**2

print("input_noise_magnitude2*block:")
(summary(model2.8_in)$coefficients[11 "Std. Error"])**2

print("positive_feedback*block:")
(summary(model2.8_in)$coefficients[12 "Std. Error"])**2

print("negative_feedback*block:")
(summary(model2.8_in)$coefficients[13 "Std. Error"])**2

print("input_noise_magnitude2*CESDR:")
(summary(model2.8_in)$coefficients[14 "Std. Error"])**2

print("input_noise_magnitude2*ExternalLC:")
(summary(model2.8_in)$coefficients[15 "Std. Error"])**2

```

## Generating simulations based on the final selected model

parametric bootstrap:
```{r bootstrap2, include=FALSE}

#confint(model2.8_in, nsim=N_iterations, parm=c('feedbackpositive', 'feedbacknegative', 'input_noise_magnitude2', 'ExternalLC', 'CESDR', 'block', 'avg_dist_trialwise', 'feedbackpositive:CESDR', 'feedbacknegative:CESDR','input_noise_magnitude2:block', 'feedbackpositive:block', 'feedbacknegative:block', 'input_noise_magnitude2:CESDR', 'input_noise_magnitude2:ExternalLC'), method='boot')

#Notiz an Nils: confint() input schon an neues Modell angepasst
```

"...bounds of the 95% confidence interval were obtained by a parametric bootstrap with {N_iterations} iterations."

```{r transform CIs2, echo=FALSE}

print("positive_feedback:")
()**2
()**2

print("negative_feedback:")
-()**2
-()**2

print("input_noise_magnitude2:")
-()**2
-()**2

print("ExternalLC:")
()**2
()**2

print("CESDR:")
-()**2
()**2

print("block:")
()**2
()**2

print("avg_dist_trialwise:")
-()**2
-()**2

print("negative_feedback*CESDR:")
()**2
()**2

print("input_noise_magnitude2*block:")
-()**2
-()**2

```

#####adjust script from here onwards!

reporting our effect:
...compared to input noise magnitude=0.5, increasing the magnitude of input noise to 2.0 significantly decreases SoC (beta=-0.080, sigma=0.002, CI=[-0.138, -0.037],  p<.001).

```{r arrow_file, include=FALSE}
# write to arrow data file
#write_feather(all_data, "data/all_data.arrow")
```

# Post-hoc tests to compare effect sizes

## negative vs. positive feedback
```{r negFeedback vs. posFeedback, include=TRUE}

bootstrap_data <- read_csv("data/bootstrap_data_soc.csv")

# columns of interest
# ß02: negative feedback
# ß03: positive feedback
# ß04: input noise:2.0

negative_feedback.diffs <- -(bootstrap_data$β02)
positive_feedback.diffs <- bootstrap_data$β03

t.test(negative_feedback.diffs, positive_feedback.diffs, alternative = "two.sided", paired=TRUE)
# we're testing whether the distribution of negative_feedback.diffs is greater than  positive_feedback.diffs

```

## input noise vs. negative feedback
```{r inputNoise vs. negFeedback, include=TRUE}

input_noise.diffs <- bootstrap_data$β04
negative_feedback.diffs <- bootstrap_data$β02

t.test(input_noise.diffs, negative_feedback.diffs, alternative = "two.sided", paired=TRUE)
# we're testing whether the distribution of input_noise.diffs is smaller than  negative_feedback.diffs

```



# Simpler models without transformatio or exploration of random slopes

## Performance
```{r performance_easy, echo=FALSE}

performance_easy <- lmer(avg_dist_trialwise ~ input_noise_magnitude * block + (1|code), data = all_data, REML = FALSE)

summary(performance_easy)

```

```{r bootstrap_performance_bounds, include=TRUE}

#confint(performance_easy, nsim=N_iterations, parm=c('input_noise_magnitude2', 'block', 'input_noise_magnitude2:block'), method='boot')

```
bounds:
                                  2.5 %     97.5 %
input_noise_magnitude2       20.3235842 22.1861712
block                        -0.6562036 -0.4002673
input_noise_magnitude2:block -0.1238160  0.2505892

```{r bootstrap_performance, include=TRUE}

bootstrap_performance_results <- bootMer(
  performance_easy,
  FUN = function(model) fixef(model),  # Extract fixed effects
  nsim = 10000,                        # Number of bootstrap samples
  use.u = FALSE,                       # Refit the model for each sample
  type = "parametric"                  # Bootstrapping type
)

# Convert bootstrap estimates to a data frame
bootstrap_performance_df <- as.data.frame(bootstrap_performance_results$t)
colnames(bootstrap_performance_df) <- names(fixef(performance_easy))  # assign variable names

# Save to CSV
write.csv(bootstrap_performance_df, "data/simple models/bootstrap_performance_results.csv", row.names = FALSE)

```


## SoC
I don't think we had an hypothesis for block affecting SoC.
```{r soc_easy, echo=FALSE}

soc_easy <- lmer(SoC ~ feedback + input_noise_magnitude + ExternalLC + InternalLC + CESDR + avg_dist_trialwise
                 #+ block
                 + feedback*input_noise_magnitude
                 + feedback*ExternalLC
                 + feedback*InternalLC
                 #+ feedback*CESDR
                 #+ #+ input_noise_magnitude*block # these weren't in our hypotheses I think...
                 + (1|code),
                 data = all_data, REML = FALSE)

summary(soc_easy)

```

```{r bootstrap_soc_bounds, include=TRUE}

confint(soc_easy, nsim=N_iterations, parm=c('input_noise_magnitude2', 'ExternalLC', 'avg_dist_trialwise'), method='boot')

```

bounds:
                             2.5 %      97.5 %
input_noise_magnitude2 -1.48673732 -1.32083086
ExternalLC              0.13646153  0.62109887
avg_dist_trialwise     -0.03710365 -0.03332908

```{r bootstrap_soc, include=TRUE}

bootstrap_soc_results <- bootMer(
  soc_easy,
  FUN = function(model) fixef(model),  # Extract fixed effects
  nsim = 10000,                        # Number of bootstrap samples
  use.u = FALSE,                       # Refit the model for each sample
  type = "parametric"                  # Bootstrapping type
)

# Convert bootstrap estimates to a data frame
bootstrap_soc_df <- as.data.frame(bootstrap_soc_results$t)
colnames(bootstrap_soc_df) <- names(fixef(soc_easy))  # assign variable names

# Save to CSV
write.csv(bootstrap_soc_df, "data/simple models/bootstrap_soc_results.csv", row.names = FALSE)

```
