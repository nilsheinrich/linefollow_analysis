---
title: "model selection"
author: "Maren Giersiepen"
date: "2025-07-10"
output: pdf_document
---

```{r global, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup, include=FALSE}

library(tidyverse)
library(lme4)
library(lmerTest)
library(sjPlot)
library(latex2exp)
library(ggplot2)
library(fitdistrplus)
library(simr)
library(arrow)
library(MASS)
library(MuMIn)

set.seed(36)
N_iterations <- 10000

```


```{r data, echo=FALSE}

#paths
trial_data_path <- paste(getwd(), "/data/trialwise_data.csv", sep="")
questionnaire_data_path <- paste(getwd(), "/data/questionnaire_scores.csv", sep="")

#load data
trial_data <- read_csv(trial_data_path)
questionnaire_data <- read_csv(questionnaire_data_path)

#factorization
trial_data$feedback <- factor(trial_data$feedback, levels = c("neutral", "positive", "negative"))
trial_data$input_noise_magnitude <- factor(trial_data$input_noise_magnitude)
trial_data$code <- factor(trial_data$code) #added as factor variable by Maren
trial_data$inpnoise_ascending <- factor(trial_data$inpnoise_ascending)
trial_data$block <- as.numeric(trial_data$block)

```

Create one dataset containing both the experimental data and the questionnaire scores.
Furthermore, exclude participant 12 and 14 from all statistical analyses.
```{r create final dataset, echo=FALSE}

#exclude pp 12 and 14 from experimental data
trial_data <- trial_data %>% filter(code != 12 & code != 14)

#rename "ID" to "code"
colnames(questionnaire_data)[colnames(questionnaire_data) == "ID"] <- "code"
all_data <- merge(trial_data, questionnaire_data, by = "code", all.x = TRUE)

```

```{r individual_feedback columns, echo=FALSE}

all_data <- all_data %>% 
  mutate(negative_feedback=case_when(
    feedback=="negative" ~ 1,
    feedback=="positive" | feedback=="neutral" ~ 0
  ))

all_data <- all_data %>% 
  mutate(positive_feedback=case_when(
    feedback=="positive" ~ 1,
    feedback=="negative" | feedback=="neutral" ~ 0
  ))

```

Final fixed effects model based on model_selection.rmd
```{r model2.6, echo=FALSE}

model2.6 <- lmer(sqrt(SoC) ~  feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1|code),
                data = all_data, REML = FALSE)

summary(model2.6)

```

## Exploring random slope effects by referring to BIC

Just entering all the fixed effects as random slopes.
```{r model2.7_1, echo=FALSE}

model2.7_1 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback + feedback + input_noise_magnitude + block + avg_dist_trialwise + input_noise_magnitude*block|code),
                data = all_data, REML = FALSE)

```
That took a while and the model is drastically overparameterized (failed to converge)... We will first eliminate interaction effect input_noise_magnitude*block

```{r model2.7_2, echo=FALSE}

model2.7_2 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback + feedback + input_noise_magnitude + block |code),
                data = all_data, REML = FALSE)

```
Detecting singularity...

Throwing out avg_dist_trialwise
```{r model2.7_3, echo=FALSE}

model2.7_3 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback + feedback + input_noise_magnitude |code),
                data = all_data, REML = FALSE)

```

Model failed to converge. Exclude feedback - this effect is not involved in any interaction. 
```{r model2.7_4, echo=FALSE}

model2.7_4 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback + input_noise_magnitude|code),
                data = all_data, REML = FALSE)

```

That one worked. Ok than let's just build single random slope models and compare those.

```{r model2.7_nfb, echo=FALSE}

model2.7_nfb <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + negative_feedback|code),
                data = all_data, REML = FALSE)

```

```{r model2.7__in, echo=FALSE}

model2.7__in <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + (1 + input_noise_magnitude|code),
                data = all_data, REML = FALSE)

```
Let's compare the non-singular models and the random intercept only one.
```{r model_comparison2, echo=FALSE}

anova(model2.6, model2.7_4, model2.7_nfb, model2.7__in)

```
The only statistic of interest for us in this output is the BIC and we're searching for the smallest BIC. mode.2.7_4 (input_noise_magnitude & negative_feedback as random slope effects) has the lowest BIC even outcompeting the random intercept only model.

We can now take a look at the effects.

```{r final selected model2, echo=FALSE}

summary(model2.7_4)

```

### Back-transformation

means:
```{r transform means2, echo=FALSE}

fixef(model2.7_4)[1]**2 # intercept
fixef(model2.7_4)[2]**2 # positive_feedback
-fixef(model2.7_4)[3]**2 # negative_feedback
-fixef(model2.7_4)[4]**2 # input_noise_magnitude2
fixef(model2.7_4)[5]**2 # ExternalLC
-fixef(model2.7_4)[6]**2 # CESDR
fixef(model2.7_4)[7]**2 # block
-fixef(model2.7_4)[8]**2 # avg_dist_trialwise
fixef(model2.7_4)[9]**2 # negative_feedback:CESDR
-fixef(model2.7_4)[10]**2 # input_noise_magnitude2:block

```

standard errors:
```{r transform sds2, echo=FALSE}

print("intercept:")
(summary(model2.7_4)$coefficients[1, "Std. Error"])**2

print("positive_feedback:")
(summary(model2.7_4)$coefficients[2, "Std. Error"])**2

print("negative_feedback:")
(summary(model2.7_4)$coefficients[3, "Std. Error"])**2

print("input_noise_magnitude2:")
(summary(model2.7_4)$coefficients[4, "Std. Error"])**2

print("ExternalLC:")
(summary(model2.7_4)$coefficients[5, "Std. Error"])**2

print("CESDR:")
(summary(model2.7_4)$coefficients[6, "Std. Error"])**2

print("block:")
(summary(model2.7_4)$coefficients[7, "Std. Error"])**2

print("avg_dist_trialwise:")
(summary(model2.7_4)$coefficients[8, "Std. Error"])**2

print("negative_feedback*CESDR:")
(summary(model2.7_4)$coefficients[9, "Std. Error"])**2

print("input_noise_magnitude2*block:")
(summary(model2.7_4)$coefficients[10, "Std. Error"])**2

```

## Generating simulations based on the final selected model

parametric bootstrap:
```{r bootstrap, include=FALSE}

bootstrap_results <- bootMer(
  model2.7_4,
  FUN = function(model) fixef(model),  # extract fixed effects
  nsim = N_iterations,                 # number of bootstrap samples
  use.u = FALSE,                       # refit the model for each sample
  type = "parametric"                  # bootstrapping type
)
# convert bootstrap estimates to a data frame
bootstrap_df <- as.data.frame(bootstrap_results$t)
colnames(bootstrap_df) <- names(fixef(model2.7_4))  # assign variable names

# save to CSV
write.csv(bootstrap_df, "bootstrap_results_extended_model.csv", row.names = FALSE)
```

running again a parametric bootstrap, also to check on the output of the function above...
```{r CIs, include=FALSE}

confint(model2.7_4, nsim=N_iterations, parm=c('feedbackpositive', 'feedbacknegative', 'input_noise_magnitude2', 'ExternalLC', 'CESDR', 'block', 'avg_dist_trialwise', 'CESDR:negative_feedback', 'input_noise_magnitude2:block'), method='boot')

```

output:
                                     2.5 %       97.5 %
feedbackpositive              0.0250211797  0.048341714
feedbacknegative             -0.1023222606 -0.040202238
input_noise_magnitude2       -0.3699995313 -0.192771884
ExternalLC                    0.0429972137  0.167330254
CESDR                        -0.0051382091  0.004348421
block                         0.0036872229  0.010000821
avg_dist_trialwise           -0.0094587220 -0.008564743
CESDR:negative_feedback      -0.0007068882  0.003268267
input_noise_magnitude2:block -0.0236261966 -0.014750268

"...bounds of the 95% confidence interval were obtained by a parametric bootstrap with {N_iterations} iterations."

```{r transform CIs, echo=FALSE}

print("positive_feedback:")
()**2
()**2

print("negative_feedback:")
-()**2
-()**2

print("input_noise_magnitude2:")
-()**2
-()**2

print("ExternalLC:")
()**2
()**2

print("CESDR:")
-()**2
()**2

print("block:")
()**2
()**2

print("avg_dist_trialwise:")
-()**2
-()**2

print("negative_feedback*CESDR:")
()**2
()**2

print("input_noise_magnitude2*block:")
-()**2
-()**2

```

reporting our effect:
...compared to input noise magnitude=0.5, increasing the magnitude of input noise to 2.0 significantly decreases SoC (beta=-0.080, sigma=0.002, CI=[-0.138, -0.037],  p<.001).

```{r arrow_file, include=FALSE}
# write to arrow data file
#write_feather(all_data, "data/all_data.arrow")
```

# Post-hoc tests to compare effect sizes

## negative vs. positive feedback
```{r negFeedback vs. posFeedback, include=TRUE}

bootstrap_data <- read_csv("data/bootstrap_data_soc.csv")

# columns of interest
# ß02: negative feedback
# ß03: positive feedback
# ß04: input noise:2.0

negative_feedback.diffs <- -(bootstrap_data$β02)
positive_feedback.diffs <- bootstrap_data$β03

t.test(negative_feedback.diffs, positive_feedback.diffs, alternative = "two.sided", paired=TRUE)
# we're testing whether the distribution of negative_feedback.diffs is greater than  positive_feedback.diffs

```

## input noise vs. negative feedback
```{r inputNoise vs. negFeedback, include=TRUE}

input_noise.diffs <- bootstrap_data$β04
negative_feedback.diffs <- bootstrap_data$β02

t.test(input_noise.diffs, negative_feedback.diffs, alternative = "two.sided", paired=TRUE)
# we're testing whether the distribution of input_noise.diffs is smaller than  negative_feedback.diffs

```

#exploratory model extensions following reviewer comments

```{r model2.7_4, echo=FALSE}

model3.expl1 <- lmer(sqrt(SoC) ~ feedback + input_noise_magnitude + ExternalLC + CESDR + block + avg_dist_trialwise 
                + negative_feedback*CESDR
                + input_noise_magnitude*block
                + input_noise_magnitude*CESDR #extension
                + feedback*block #extension
                + (1 + negative_feedback + input_noise_magnitude|code),
                data = all_data, REML = FALSE)

summary(model3.expl1)

```
We observe a significant negative interaction of positive feedback and block, indicating that the effect of positive feedback on SoC levels off over time.
We see a siginficant negative interaction of input noise and CESDR, indicating that the effect of input noise (increasing input noise reduces SoC) decreases with higher depression scores.

```{r bootstrap2, include=FALSE}

bootstrap_results_expl <- bootMer(
  model3.expl1,
  FUN = function(model) fixef(model),  # extract fixed effects
  nsim = N_iterations,                 # number of bootstrap samples
  use.u = FALSE,                       # refit the model for each sample
  type = "parametric"                  # bootstrapping type
)
# convert bootstrap estimates to a data frame
bootstrap_df_expl <- as.data.frame(bootstrap_results_expl$t)
colnames(bootstrap_df_expl) <- names(fixef(model3.expl1))  # assign variable names

# save to CSV
write.csv(bootstrap_df_expl, "bootstrap_results_explorative_model.csv", row.names = FALSE)
```

```{r CIs2, include=FALSE}

confint(model3.expl1, nsim=N_iterations, parm=c('feedbackpositive', 'feedbacknegative', 'input_noise_magnitude2', 'ExternalLC', 'CESDR', 'block', 'avg_dist_trialwise', 'CESDR:negative_feedback', 'input_noise_magnitude2:block', 'input_noise_magnitude2:CESDR', 'feedbackpositive:block', 'feedbacknegative:block'), method='boot')

```

output:
                                     2.5 %       97.5 %
feedbackpositive              0.0588645082  0.109842708
feedbacknegative             -0.1317334386 -0.056277349
input_noise_magnitude2       -0.2514920216  0.008583518
ExternalLC                    0.0432915920  0.167390077
CESDR                        -0.0002837581  0.011821129
block                         0.0051872106  0.013773301
avg_dist_trialwise           -0.0094380585 -0.008543980
CESDR:negative_feedback       0.0001314951  0.004165048
input_noise_magnitude2:block -0.0236197526 -0.014760653
input_noise_magnitude2:CESDR -0.0221399435 -0.005125305
feedbackpositive:block       -0.0156583980 -0.005609648
feedbacknegative:block       -0.0020555077  0.007781237


"...bounds of the 95% confidence interval were obtained by a parametric bootstrap with {N_iterations} iterations."

```{r transform CIs2, echo=FALSE}

print("positive_feedback:")
()**2
()**2

print("negative_feedback:")
-()**2
-()**2

print("input_noise_magnitude2:")
-()**2
-()**2

print("ExternalLC:")
()**2
()**2

print("CESDR:")
-()**2
()**2

print("block:")
()**2
()**2

print("avg_dist_trialwise:")
-()**2
-()**2

print("negative_feedback*CESDR:")
()**2
()**2

print("input_noise_magnitude2*block:")
-()**2
-()**2

print('input_noise_magnitude2:CESDR:')

print('feedbackpositive:block:')

print('feedbacknegative:block:')

```

In this model, the interaction of CESDR and negative feedback turns out significant again. Lets test whether the effect of CESDR on SoC is stronger for a) feedback, or b) input noise

```{r CESDR on inputNoise vs. negFeedback, include=TRUE}

#analysis still needs to be conducted - compare bootstrapped effect sizes?

```

As a sense check - check the correlation of CESDR and the within-subject effect of negative feedback
```{r correlation CESDR negFeedback, include=TRUE}

#take difference score of negative-neutral feedback
#obtain feedback effect for each participant
feedback_effect <- aggregate(SoC ~ code + feedback, data = all_data, FUN = mean)
feedback_effect_wide <- reshape(feedback_effect, idvar = "code", timevar = "feedback", direction = "wide")
feedback_effect_wide$fb_diff_neg_neu <- feedback_effect_wide$SoC.negative - feedback_effect_wide$SoC.neutral #contrast negative-neutral

feedback_effect_wide$SoC.nonnegative <- rowMeans(
  feedback_effect_wide[, c("SoC.neutral", "SoC.positive")], na.rm = TRUE
)
feedback_effect_wide$fb_diff_neg_vs_nonneg <- feedback_effect_wide$SoC.negative - feedback_effect_wide$SoC.nonnegative #contrast negative-[neutral+positive]

#merge with cesdr scores
cesdr_scores <- unique(all_data[, c("code", "CESDR")])
fb_corr_data <- merge(feedback_effect_wide, cesdr_scores, by = "code")

#perform standard correlation test negative-neutral
cor.test(fb_corr_data$fb_diff_neg_neu, fb_corr_data$CESDR)

```
The correlation analysis indicates a significant positive correlation of CESDR scores and the difference between neutral and negative feedback - indicating a stronger effect of negative feedback with higher depression scores.



To-Do: 
1) Bootstrap final model results (from main model) and create CIs.    ✓
2) compare relative positive vs. negative feedback effect size on SoC 
3) compare input noise vs. feedback effect size on soc
4) bootstrap exploratory extended model and create CIs                ✓
5) compare interaction effect size CESDR:neg_fb and CESDR:input_noise
6) ..?