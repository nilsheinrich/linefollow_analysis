---
title: "linefollow_analysis"
author: "Nils Wendel Heinrich"
date: "2024-07-17"
output: pdf_document
---

```{r global, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r setup, include=FALSE}

library(tidyverse)
library(lme4)
library(lmerTest)
library(sjPlot)
library(latex2exp)
library(ggplot2)
library(fitdistrplus)
library(simr)
library(arrow)
library(MASS)
library(MuMIn)

set.seed(36)
N_iterations <- 100

```


```{r data, echo=FALSE}

#paths
trial_data_path <- paste(getwd(), "/data/trialwise_data.csv", sep="")
questionnaire_data_path <- paste(getwd(), "/data/questionnaire_scores.csv", sep="")

#load data 
trial_data <- read_csv(trial_data_path)
questionnaire_data <- read_csv(questionnaire_data_path)

#Maren
#trial_data <- read.csv("H:/Maren/Experiments/COAF/Main/data/experiment_preprocessed/trialwise_data.csv")
#questionnaire_data <- read.csv("H:/Maren/Experiments/COAF/Main/data/questionnaire_preprocessed/questionnaire_scores.csv")

# factorization
trial_data$feedback <- factor(trial_data$feedback, levels = c("neutral", "positive", "negative"))
trial_data$input_noise_magnitude <- factor(trial_data$input_noise_magnitude)
trial_data$code <- factor(trial_data$code) #added as factor variable by Maren

#? also code input_noise_ascending as factor?

```

Create one dataset containing both the experimental data and the questionnaire scores.
Furthermore, exclude participant 12 and 14 from all statistical analyses.
```{r create final dataset, echo=FALSE}

#exclude pp 12 and 14 from experimental data
trial_data <- trial_data %>% filter(code != 12 & code != 14)

#rename "ID" to "code"
colnames(questionnaire_data)[colnames(questionnaire_data) == "ID"] <- "code"
all_data <- merge(trial_data, questionnaire_data, by = "code", all.x = TRUE)

#add categorical depression variable with participants scorint >= 16 (critical) scoring 1 and participants scoring < 16 scoring 0
all_data$CESDR_cat <- ifelse(all_data$CESDR >= 16, 1, 0)
all_data$CESDR_cat <- as.factor(all_data$CESDR_cat )

```

# H1: Perceived control over spaceship trajectory is expected to be higher for low-input noise spaceship than high-input noise spaceship.

Distributional analysis:
```{r h1_boxcox, echo=FALSE}

lambda_soc <- boxcox(lm(all_data$SoC ~ 1))

lambda_soc$x[which(lambda_soc$y == max(lambda_soc$y))]

```
referring to:
https://www.statisticshowto.com/probability-and-statistics/normal-distributions/box-cox-transformation/

lambda, the expected value is closer to 0.5 than 1 (1 would be ideal, suggesting no transformation). 0.5 implies a âˆš transformation, which we will apply. 
"a boxcox distributional analysis implied a square root transformation of the predicted variable"


### null model 
predict mean (independent of any predictors) SoC rating
```{r h1_lm, echo=FALSE}
soc_null <- lmer(sqrt(SoC) ~ 1 + (1|code), data = all_data)
summary(soc_null)
```
How much variance in SoC Ratings is explained by "Code"?
```{r h1_lm, echo=FALSE}

fixef(soc_null)**2
0.02771**2

```

...

```{r h1_lm, echo=FALSE}

h1_lm <- lmer(sqrt(SoC) ~ input_noise_magnitude + (1|code), data = all_data)

summary(h1_lm)

```
We detect a significant effect for input_noise_magnitude on SoC (p<.001).

proportional variance explained by our random effect (participant) code
```{r h1_icc, echo=FALSE}

#Intraclass correlation coefficient
performance::icc(h1_lm)

#other options of quantifying effect size in our data



```

```{r h1_r_squared, echo=FALSE}

r_full <- r.squaredGLMM(h1_lm)
# R2m is variance of fixed effects

r_null <- r.squaredGLMM(soc_null)

fixed_effects_variance <- r_full[1,"R2m"] - r_null[1,"R2m"]
fixed_effects_variance
```

```{r h1_coefs, echo=FALSE}

#intercept:
fixef(h1_lm)[1]**2
# mean response across all participants (code) when input_noise_magnitude = 0.5

# slope
fixef(h1_lm)[2]**2
# mean difference in response across all participants (code) when input_noise_magnitude jumps from 0.5 to 2.0

```

parametric bootstrap:
```{r h1_bootstrap, echo=FALSE}

confint(h1_lm, nsim=N_iterations, parm=c('input_noise_magnitude2'), method='boot')

```

bounds of the 95% confidence interval were obtained by a parametric bootstrap with {N_iterations} iterations.

reporting our effect:
...increasing magnitude of input noise significantly decreases SoC (beta={fixef(h1_lm)[2]**2}, CI=[-0.5528189, -0.5290939],  p<.001).


# H2: Perceived control is higher during trials with positive compared to neutral and negative feedback (testing illusion of control)

```{r h2_lm, echo=FALSE}

h2_lm <- lmer(sqrt(SoC) ~ feedback + (1|code), data = all_data)

summary(h2_lm)

```

```{r h2_r_squared, echo=FALSE}

r_full <- r.squaredGLMM(h2_lm)
# R2m is variance of fixed effects

r_null <- r.squaredGLMM(soc_null)

fixed_effects_variance <- r_full[1,"R2m"] - r_null[1,"R2m"]
fixed_effects_variance
```

```{r h2_coefs, echo=FALSE}

#intercept:
fixef(h2_lm)[1]**2
# mean response across all participants (code) when feedback is neutral

# slopes:
fixef(h2_lm)[2]**2
# mean difference in response across all participants (code) when feedback is positive compared to neutral

fixef(h2_lm)[3]**2
# mean difference in response across all participants (code) when feedback is negative compared to neutral

```

parametric bootstrap:
```{r h2_bootstrap, echo=FALSE}

confint(h2_lm, nsim=N_iterations, parm=c('feedbackpositive', 'feedbacknegative'), method='boot')

```

# H3: Effect of feedback on SoC ratings is higher in high input noise blocks (interaction effect feedback x input noise => SoC)

```{r h3_lm, echo=FALSE}

# full model
h3_lm <- lmer(sqrt(SoC) ~ feedback * input_noise_magnitude + (1|code), data = all_data)

summary(h3_lm)

```

:( no significant interaction effects.

### Changing contrasts so to compare positive and negative feedback:

```{r h3_lm_valence, echo=FALSE}

# new factorization
trial_data$feedback <- factor(trial_data$feedback, levels = c("positive", "negative", "neutral"))

# full model
h3_lm_valence <- lmer(sqrt(SoC) ~ feedback * input_noise_magnitude + (1|code), data = all_data)

summary(h3_lm_valence)

# old contrasts
trial_data$feedback <- factor(trial_data$feedback, levels = c("neutral", "positive", "negative"))

```

```{r h3_coefs, echo=FALSE}

#intercept:
fixef(h3_lm)[1]**2
# mean response across all participants (code) when feedback is neutral

# slopes:
fixef(h3_lm)[5]**2
# ...

fixef(h3_lm)[6]**2
# ...

```

parametric bootstrap:
```{r h3_bootstrap, echo=FALSE}

confint(h3_lm, nsim=N_iterations, parm=c('feedbackpositive', 'feedbacknegative', 'input_noise_magnitude2', 'feedbackpositive:input_noise_magnitude2', 'feedbacknegative:input_noise_magnitude2'), method='boot')

```

# H4: Performance (line deviation) is higher for low-input noise spaceship than high-input noise spaceship

Distributional analysis:
```{r avg_boxcox, echo=FALSE}

lambda_avg <- boxcox(lm(all_data$avg_dist_trialwise ~ 1))

lambda_avg$x[which(lambda_avg$y == max(lambda_avg$y))]

```

The expected value, lambda is close to 0, which implies a log transformation.

### null model
```{r h4_lm, echo=FALSE}
dist_null <- lmer(log(avg_dist_trialwise) ~ 1 + (1|code), data = all_data)
```



```{r h4_lm, echo=FALSE}

# full model
h4_lm <- lmer(log(avg_dist_trialwise) ~ input_noise_magnitude + (1|code), data = all_data)

summary(h4_lm)

```

```{r h4_coefs, echo=FALSE}

#intercept:
exp(fixef(h4_lm)[1])
# ...

# slopes:
exp(fixef(h4_lm)[2])
# ...

```

proportional variance explained by our random effect (participant) code
```{r h4_icc, echo=FALSE}

#Intraclass correlation coefficient
performance::icc(h4_lm)

```

parametric bootstrap:
```{r h4_bootstrap, echo=FALSE}

confint(h4_lm, nsim=N_iterations, parm=c('input_noise_magnitude2'), method='boot')

```

# H5: Participants experience greater sense of control during more accurate trials (lower line deviations)

```{r h5_lm, echo=FALSE}

# full model
h5_lm <- lmer(sqrt(SoC) ~ avg_dist_trialwise + (1|code), data = all_data)

summary(h5_lm)

```
We detect a significant effect of performance on SoC (p<.001).

```{r h5_r_squared, echo=FALSE}

r_full <- r.squaredGLMM(h5_lm)
# R2m is variance of fixed effects

r_null <- r.squaredGLMM(soc_null)

fixed_effects_variance <- r_full[1,"R2m"] - r_null[1,"R2m"]
fixed_effects_variance
```

```{r h5_coefs, echo=FALSE}

#intercept:
fixef(h5_lm)[1]**2
# mean SoC across all participants (code) with average line distance = 0

# slope
fixef(h5_lm)[2]**2
# mean difference in response across all participants (code) when average line distance increases by one standard deviation

```
proportional variance explained by our random effect (participant) code
```{r h5_icc, echo=FALSE}

#Intraclass correlation coefficient
performance::icc(h5_lm)

```

parametric bootstrap:
```{r h5_bootstrap, echo=FALSE}

confint(h5_lm, nsim=N_iterations, parm=c('avg_dist_trialwise'), method='boot')

```

bounds of the 95% confidence interval were obtained by a parametric bootstrap with {N_iterations} iterations.

reporting our effect:
...increasing magnitude of input noise significantly decreases SoC (beta={fixef(h5_lm)[2]**2}, CI=[-0.02, -0.02],  p<.001).


# Exploratory Analysis: Do participants improve performance (i.e., reduce avg. distance to the line) throughout block/task progression (in high and low input noise blocks)?

```{r e1_lm, echo=FALSE}

# full model
# full model
e1_lm <- lmer(log(avg_dist_trialwise) ~ inpnoise_ascending + (1|code), data = all_data)

summary(e1_lm)

```
We detect a significant effect of block progression on performance (p<.001).

```{r e1_r_squared, echo=FALSE}

r_full <- r.squaredGLMM(e1_lm)
# R2m is variance of fixed effects

r_null <- r.squaredGLMM(dist_null)

fixed_effects_variance <- r_full[1,"R2m"] - r_null[1,"R2m"]
fixed_effects_variance
```

```{r e1_coefs, echo=FALSE}

#intercept:
exp(fixef(e1_lm)[1])
# mean line distance across all participants, independent of block number (?!)

# slopes:
exp(fixef(e1_lm)[2])
# mean decrease in line distance across all participants (code) when progressing by one block / one standard deviation?

```
proportional variance explained by our random effect (participant) code
```{r e1_icc, echo=FALSE}

#Intraclass correlation coefficient
performance::icc(e1_lm)

```

parametric bootstrap:
```{r e1_bootstrap, echo=FALSE}

confint(e1_lm, nsim=N_iterations, parm=c('inpnoise_ascending'), method='boot')

```

bounds of the 95% confidence interval were obtained by a parametric bootstrap with {N_iterations} iterations.

reporting our effect:
...increasing magnitude of input noise significantly decreases SoC (beta={fixef(h5_lm)[2]**2}, CI=[-0.02, -0.02],  p<.001).

```{r e2_lm, echo=FALSE}

#add an interaction term testing for differences in improvement across noise conditions

e2_lm <- lmer(log(avg_dist_trialwise) ~ inpnoise_ascending * input_noise_magnitude + (1|code), data = all_data)

summary(e2_lm)

```

```{r post-hoc contrasts, echo=FALSE}

#high input noise
model_high <- lmer(log(avg_dist_trialwise) ~ inpnoise_ascending + (1|code), data = subset(all_data, input_noise_magnitude == "2"))
summary(model_high)

#low input noise
model_low <- lmer(log(avg_dist_trialwise) ~ inpnoise_ascending + (1|code), data = subset(all_data, input_noise_magnitude == "0.5"))
summary(model_low)

```
The effect of block progression on performance seems to be larger in low compared to high input noise blocks.

# Exploratory Analysis: Are higher depression scores associated with reduced SoC judgments?

```{r e3_lm, echo=FALSE}

# full model - quantitative CESDR variable
e3_lm <- lmer(sqrt(SoC) ~ CESDR + (1|code), data = all_data)

summary(e3_lm)

# full model - categorical CESDR variable
e3_cat_lm <- lmer(sqrt(SoC) ~ CESDR_cat + (1|code), data = all_data)

summary(e3_cat_lm)

```
# Exploratory Analysis: Are ILC scores associated with increased SoC judgments?

```{r e4_lm, echo=FALSE}

# full model
e4_lm <- lmer(sqrt(SoC) ~ InternalLC * feedback + (1|code), data = all_data)

summary(e4_lm)

```
# Exploratory Analysis: Are ELC scores associated with reduced SoC judgments?

```{r e5_lm, echo=FALSE}

# full model
e5_lm <- lmer(sqrt(SoC) ~ ExternalLC * feedback + (1|code), data = all_data)

summary(e5_lm)

```
External locus of control but not internal locus of control scores or depression scores are associated with SoC scores. Individuals with higher values on the external locus of control scale tend to provide higher SoC ratings. 


# Exploratory Analysis: Do feedback, input noise, and questionnaire data interact on soc 

```{r e_further_lm, echo=FALSE}

# full model
e6_lm <- lmer(sqrt(SoC) ~ ExternalLC * feedback * input_noise_magnitude + (1|code), data = all_data)

summary(e6_lm)

# full model
e7_lm <- lmer(sqrt(SoC) ~ InternalLC * feedback * input_noise_magnitude + (1|code), data = all_data)

summary(e7_lm)


# full model
e8_lm <- lmer(sqrt(SoC) ~ CESDR * feedback * input_noise_magnitude + (1|code), data = all_data)

summary(e8_lm)

```

The influence of participants' questionnaire scores on SoC Ratings depends on input noise condition.
Furthermore, the influence of Depression Score on SoC Rating depends on feedback type.

# Follow-up Analysis: Create dummy-coded variable with depression scores >= 16 (depression) and < 16 (no depression)

```{r e_further_lm, echo=FALSE}

# full model
e6_lm <- lmer(sqrt(SoC) ~ ExternalLC * feedback * input_noise_magnitude + (1|code), data = all_data)

summary(e6_lm)

# full model
e7_lm <- lmer(sqrt(SoC) ~ InternalLC * feedback * input_noise_magnitude + (1|code), data = all_data)

summary(e7_lm)


# full model
e8_lm <- lmer(sqrt(SoC) ~ CESDR * feedback * input_noise_magnitude + (1|code), data = all_data)

summary(e8_lm)

```